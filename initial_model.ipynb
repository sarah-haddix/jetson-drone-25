{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.6.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.21.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torchvision-0.21.0-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchvision\n",
      "Successfully installed torchvision-0.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataset\n",
    "\n",
    "## help from: https://www.learnpytorch.io/03_pytorch_computer_vision/, chatGPT\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "## Question - What other transforms (if any) should I use?\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
    "])\n",
    "\n",
    "path_train = \"/Users/sarahhaddix/code/aiclub/jetsondrone/Training\"\n",
    "path_test = \"/Users/sarahhaddix/code/aiclub/jetsondrone/Test\"\n",
    "\n",
    "train_data = datasets.ImageFolder(root=path_train, transform=transform)\n",
    "test_data = datasets.ImageFolder(root=path_test, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39375, 8617)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data) # There are 39375 training images and 8617 test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.folder.ImageFolder"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper-parameters\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "## Use Apple Silicon GPU\n",
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 254, 254])\n",
      "Image 0 dimensions: torch.Size([3, 254, 254])\n",
      "Image 1 dimensions: torch.Size([3, 254, 254])\n",
      "Image 2 dimensions: torch.Size([3, 254, 254])\n",
      "Image 3 dimensions: torch.Size([3, 254, 254])\n",
      "Image 4 dimensions: torch.Size([3, 254, 254])\n",
      "Image 5 dimensions: torch.Size([3, 254, 254])\n",
      "Image 6 dimensions: torch.Size([3, 254, 254])\n",
      "Image 7 dimensions: torch.Size([3, 254, 254])\n",
      "Image 8 dimensions: torch.Size([3, 254, 254])\n",
      "Image 9 dimensions: torch.Size([3, 254, 254])\n",
      "Image 10 dimensions: torch.Size([3, 254, 254])\n",
      "Image 11 dimensions: torch.Size([3, 254, 254])\n",
      "Image 12 dimensions: torch.Size([3, 254, 254])\n",
      "Image 13 dimensions: torch.Size([3, 254, 254])\n",
      "Image 14 dimensions: torch.Size([3, 254, 254])\n",
      "Image 15 dimensions: torch.Size([3, 254, 254])\n",
      "Image 16 dimensions: torch.Size([3, 254, 254])\n",
      "Image 17 dimensions: torch.Size([3, 254, 254])\n",
      "Image 18 dimensions: torch.Size([3, 254, 254])\n",
      "Image 19 dimensions: torch.Size([3, 254, 254])\n",
      "Image 20 dimensions: torch.Size([3, 254, 254])\n",
      "Image 21 dimensions: torch.Size([3, 254, 254])\n",
      "Image 22 dimensions: torch.Size([3, 254, 254])\n",
      "Image 23 dimensions: torch.Size([3, 254, 254])\n",
      "Image 24 dimensions: torch.Size([3, 254, 254])\n",
      "Image 25 dimensions: torch.Size([3, 254, 254])\n",
      "Image 26 dimensions: torch.Size([3, 254, 254])\n",
      "Image 27 dimensions: torch.Size([3, 254, 254])\n",
      "Image 28 dimensions: torch.Size([3, 254, 254])\n",
      "Image 29 dimensions: torch.Size([3, 254, 254])\n",
      "Image 30 dimensions: torch.Size([3, 254, 254])\n",
      "Image 31 dimensions: torch.Size([3, 254, 254])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print(images.shape) # ([32, 3, 254, 254])\n",
    "    for i in range(images.size(0)):\n",
    "        print(f\"Image {i} dimensions: {images[i].shape}\")\n",
    "    break\n",
    "\n",
    "# Images are 3x254x254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fire': 0, 'No_Fire': 1}\n"
     ]
    }
   ],
   "source": [
    "# Check the class indices\n",
    "print(train_data.class_to_idx)  # Output: {'Fire': 0, 'No_Fire': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODELS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple CNN from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "## originally for 10 category classification, added last linear layer\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # convolutional layer, takes 3 channel input, 6 output channels => outputs 6 feature maps. 5x5 kernel size\n",
    "        self.pool = nn.MaxPool2d(2, 2) # 2x2 window with a stride of 2. Chooses max value in that 2x2 window and that becomes the output\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # conv, takes 6 channel input, 16 channel output, 5x5 kernel\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5 * 144, 120) # 16*5*5*144 -> 120\n",
    "        self.fc2 = nn.Linear(120, 84) # 120 -> 84\n",
    "        self.fc3 = nn.Linear(84, 10) # 84 -> 10\n",
    "        self.fc4 = nn.Linear(10, 2) # 10 -> 2 (binary classification)\n",
    "\n",
    "    # batch_size=32\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # [(32, 6, 254, 254)] -> [(32, 6, 125, 125)]\n",
    "        x = self.pool(F.relu(self.conv2(x))) # [(32, 6, 125, 125)] -> [(32, 16, 60, 60)]\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch (becomes 32 1d vectors) [(32, 16, 60, 60)] -> [(32, 57600)]\n",
    "        x = F.relu(self.fc1(x)) # [(32, 57600)] -> [(32, 120)]\n",
    "        x = F.relu(self.fc2(x)) # [(32,120)] -> [(32, 84)]\n",
    "        x = self.fc3(x) #[(32, 84)] -> [(32, 10)]\n",
    "        x = self.fc4(x) #[(32, 10)] -> [(32, 2)]\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net().to(device) # send model to gpu\n",
    "\n",
    "# n feature maps - convolutional layer will learn n filters (different kernels), each of which convolves over the image to produce \n",
    "# a different feature map. Size of kernel is 3rd parameter - i.e. the first layer uses a 5x5 kernel\n",
    "\n",
    "# Max pooling is a down-sampling operation that reduces the size of the input feature maps while retaining the most important \n",
    "# information. It does this by sliding a window (or kernel) over the input and taking the maximum value within that window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.000\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1], device='mps:0')\n",
      "tensor([0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 1], device='mps:0')\n",
      "[1,   301] loss: 0.003\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0], device='mps:0')\n",
      "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 1, 1, 0, 1, 0, 0, 0], device='mps:0')\n",
      "[1,   601] loss: 0.003\n",
      "tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1], device='mps:0')\n",
      "tensor([0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1], device='mps:0')\n",
      "[1,   901] loss: 0.003\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0], device='mps:0')\n",
      "tensor([0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 1, 1, 0, 0], device='mps:0')\n",
      "[1,  1201] loss: 0.002\n",
      "tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0], device='mps:0')\n",
      "tensor([1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0], device='mps:0')\n",
      "[2,     1] loss: 0.000\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0], device='mps:0')\n",
      "tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 0, 0], device='mps:0')\n",
      "[2,   301] loss: 0.002\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0], device='mps:0')\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
      "        1, 0, 1, 0, 0, 1, 0, 0], device='mps:0')\n",
      "[2,   601] loss: 0.003\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1], device='mps:0')\n",
      "tensor([0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1], device='mps:0')\n",
      "[2,   901] loss: 0.003\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 1, 0], device='mps:0')\n",
      "tensor([0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 0, 0, 1, 0], device='mps:0')\n",
      "[2,  1201] loss: 0.002\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 0, 0], device='mps:0')\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 0, 0], device='mps:0')\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0): # my guess is that there's 39375/32 = 1136.7 = 1137 mini-batches\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Move inputs and labels to the MPS device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        loss = criterion(outputs, labels) # [(32, 2)] and [(32)]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 300 == 0:    # print every 300 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "            print(labels)\n",
    "            print(preds)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './init_cnn_2.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 43 %\n"
     ]
    }
   ],
   "source": [
    "# from https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load net from saved weights\n",
    "net = Net().to(device)\n",
    "net.load_state_dict(torch.load('./init_cnn_2.pth', weights_only=True))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader, 0): # doing testing in batches of 32\n",
    "        images, labels = data # images: ([32, 3, 254, 254]), labels: ([32])\n",
    "        inputs, labels = images.to(device), labels.to(device) # added myself\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(inputs) # ([32, 2])\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        predicted = torch.argmax(outputs, 1) # predicted: ([32])\n",
    "        total += labels.size(0) # (32)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        \"\"\"\n",
    "        # Visualization: Display the first few images in the batch\n",
    "        num_images_to_display = 5  # Number of images to display\n",
    "        if True:  # Only visualize the first batch\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            for j in range(num_images_to_display):\n",
    "                # Convert the image tensor to numpy for visualization\n",
    "                image = images[j].cpu().numpy().transpose(1, 2, 0)  # Change from (C, H, W) to (H, W, C)\n",
    "                image = (image - image.min()) / (image.max() - image.min())  # Normalize to [0, 1]\n",
    "                \n",
    "                plt.subplot(1, num_images_to_display, j + 1)\n",
    "                plt.imshow(image)\n",
    "                plt.title(f'True: {labels[j].item()}, Pred: {predicted[j].item()}')\n",
    "                plt.axis('off')\n",
    "            plt.show()\n",
    "        \"\"\"\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "## Use Apple Silicon GPU\n",
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "## More complicated CNN - based on VGG-16 - https://medium.com/@siddheshb008/vgg-net-architecture-explained-71179310050f\n",
    "\n",
    "# images are (32, 3, 254, 254), have been resized to (32, 3, 224, 224)\n",
    "class CNN_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1a = nn.Conv2d(3, 64, 3, padding='same') # 3 channel input, 64 channel output, 3x3 kernel\n",
    "        self.conv1b = nn.Conv2d(64, 64, 3, padding='same')\n",
    "        self.pool = nn.MaxPool2d(2, 2) # 2x2 window with a stride of 2. Chooses max value in that 2x2 window and that becomes the output\n",
    "        self.conv2a = nn.Conv2d(64, 128, 3, padding='same') \n",
    "        self.conv2b = nn.Conv2d(128, 128, 3, padding='same')\n",
    "        self.conv3a = nn.Conv2d(128, 256, 3, padding='same')\n",
    "        self.conv3b = nn.Conv2d(256, 256, 3, padding='same')\n",
    "        self.conv4a = nn.Conv2d(256, 512, 3, padding='same')\n",
    "        self.conv4b = nn.Conv2d(512, 512, 3, padding='same')\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(25088, 4096) \n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 1000) \n",
    "        self.fc4 = nn.Linear(1000, 2) # 10 -> 2 (binary classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('before layer 1')\n",
    "        print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv1b(F.relu(self.conv1a(x))))) # layer 1\n",
    "        print('after layer 1')\n",
    "        print(x.shape)\n",
    "        \"\"\"\n",
    "        print('xxxx')\n",
    "        x = self.conv2a(x)\n",
    "        print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv2b(x)\n",
    "        print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        print(x.shape)\n",
    "        print('xxx')\n",
    "        \"\"\"\n",
    "        x = self.pool(F.relu(self.conv2b(F.relu(self.conv2a(x))))) # layer 2\n",
    "        print('after layer 2')\n",
    "        print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv3b(F.relu(self.conv3b(F.relu(self.conv3a(x))))))) # layer 3\n",
    "        print('after layer 3')\n",
    "        print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv4b(F.relu(self.conv4b(F.relu(self.conv4a(x))))))) # layer 4\n",
    "        print('after layer 4')\n",
    "        print(x.shape)\n",
    "        x = self.pool(F.relu(self.conv4b(F.relu(self.conv4b(F.relu(self.conv4b(x))))))) # layer 5\n",
    "\n",
    "        print('before flatten')\n",
    "        print(x.shape)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch (becomes 32 1d vectors) [(32, 16, 60, 60)] -> [(32, 57600)]\n",
    "        print(x.shape)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        \n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        x = self.fc4(x) \n",
    "        return x\n",
    "\n",
    "\n",
    "net = CNN_2().to(device) # send model to gpu\n",
    "\n",
    "# n feature maps - convolutional layer will learn n filters (different kernels), each of which convolves over the image to produce \n",
    "# a different feature map. Size of kernel is 3rd parameter - i.e. the first layer uses a 5x5 kernel\n",
    "\n",
    "# Max pooling is a down-sampling operation that reduces the size of the input feature maps while retaining the most important \n",
    "# information. It does this by sliding a window (or kernel) over the input and taking the maximum value within that window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data + transforms\n",
    "\n",
    "## Import dataset\n",
    "\n",
    "## help from: https://www.learnpytorch.io/03_pytorch_computer_vision/, chatGPT\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "## Question - What other transforms (if any) should I use?\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n",
    "])\n",
    "\n",
    "path_train = \"/Users/sarahhaddix/code/aiclub/jetsondrone/Training\"\n",
    "path_test = \"/Users/sarahhaddix/code/aiclub/jetsondrone/Test\"\n",
    "\n",
    "train_data = datasets.ImageFolder(root=path_train, transform=transform)\n",
    "test_data = datasets.ImageFolder(root=path_test, transform=transform)\n",
    "\n",
    "# There are 39375 training images and 8617 test images\n",
    "\n",
    "## Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## Hyper-parameters\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before layer 1\n",
      "torch.Size([32, 3, 224, 224])\n",
      "after layer 1\n",
      "torch.Size([32, 64, 112, 112])\n",
      "after layer 2\n",
      "torch.Size([32, 128, 56, 56])\n",
      "after layer 3\n",
      "torch.Size([32, 256, 28, 28])\n",
      "after layer 4\n",
      "torch.Size([32, 512, 14, 14])\n",
      "before flatten\n",
      "torch.Size([32, 512, 7, 7])\n",
      "torch.Size([32, 25088])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.75 GB, other allocations: 4.08 MB, max allowed: 18.13 GB). Tried to allocate 392.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[341], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels) \u001b[38;5;66;03m# [(32, 2)] and [(32)]\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    628\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 17.75 GB, other allocations: 4.08 MB, max allowed: 18.13 GB). Tried to allocate 392.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0): # my guess is that there's 39375/32 = 1136.7 = 1137 mini-batches\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Move inputs and labels to the MPS device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels) # [(32, 2)] and [(32)]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 300 == 0:    # print every 300 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-tuned ConvNet (https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "# freezing all weights of ConvNet except for final fully connected layer\n",
    "\n",
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "path_train = \"/Users/sarahhaddix/code/aiclub/jetsondrone/Training\"\n",
    "path_test = \"/Users/sarahhaddix/code/aiclub/jetsondrone/Test\"\n",
    "\n",
    "train_data = datasets.ImageFolder(root=path_train, transform=transform)\n",
    "test_data = datasets.ImageFolder(root=path_test, transform=transform)\n",
    "\"\"\"\n",
    "\n",
    "data_dir = 'data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# We want to be able to train our model on an `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n",
    "# such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Resnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pyramid vision transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decide serverity of fire? Also, images all seem to be in snow\n",
    "# VGG architecutre\n",
    "# BCEWithlogits -> use torch.round(torch.sigmoid(y_logits))\n",
    "# Data augmentation - zoom in on fire, other augmentations\n",
    "# conv, batchnorm2, relu, conv, batchnorm, maxpool2d (he did 2 of these, skip layer, 2 more)\n",
    "# backbone - put it through the backbone first, then through other stuff. Mobilenet backbone might be pretty good\n",
    "# VGA monitor - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparison of models\n",
    "# first CNN fucking sucked"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
